{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WORKING_MSTC_Keras_RNN_3_Word_Embeddings_tSNE.ipynb","version":"0.3.2","provenance":[{"file_id":"1ossNQSrXyvkmF7ZvZr7QPryk49A9tpCX","timestamp":1521206578358}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"dUNKDytx2E6E","colab_type":"text"},"cell_type":"markdown","source":["# <font color=  #C70039 > Adding t-SNE to visualize  </font> <font color= #13c113  >Word Embeddings</font> with Keras\n","\n","![Deep Learning with Python](https://images-na.ssl-images-amazon.com/images/I/41DWjHboiyL._SX258_BO1,204,203,200_.jpg)\n","\n","\n","## Adapted from:\n","\n","### [Section 6.1-using-word-embeddings](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb)\n","\n","## By François Chollet\n","\n","### And [classifying-yelp-review-comments-using-lstm-and-word-embeddings](https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b)\n","\n","## By Sabber Ahamed\n","<br>\n","\n","\n","# * [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT: <font size=5 color='green'>Deep Learning with Tensorflow & Keras</font>"]},{"metadata":{"id":"d7BYb9WvopbE","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## A popular and powerful way to associate a vector with a word is the use of dense \"word vectors\", also called \"word embeddings\".\n","\n","![Word embeddings](https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png)\n","\n","from François Chollet book.keras.io\n","\n","- #### One-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the number of words in the vocabulary), \"word embeddings\" are low-dimensional floating point vectors (i.e. \"dense\" vectors, as opposed to sparse vectors).\n","- #### Unlike one-hot encoding, word embeddings are learned from data.\n","\n","It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 token in this case). So, word embeddings pack more information into far fewer dimensions."]},{"metadata":{"id":"wNYtq_MIa3t4","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","### There are two ways to obtain word embeddings:\n","\n","-    Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). In this setup, you would start with random word vectors, then **learn your word vectors in the same way that you learn the weights of a neural network**.\n","\n","<br>\n","-    Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. These are called **\"pre-trained word embeddings\"**.\n"]},{"metadata":{"id":"HAQNnW4qbYkO","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","## Learning word embeddings with the $Embedding$ layer\n","\n","- It is reasonable to learn a new embedding space with every new task.\n","- Thankfully, backpropagation makes this really easy,\n","- and Keras makes it even easier.\n","\n","      It's just about learning the weights of a layer: the Embedding layer."]},{"metadata":{"id":"WwlUcixycE8S","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow\n","from tensorflow import keras\n","\n","from keras.layers import Embedding\n","\n","# The Embedding layer takes at least two arguments:\n","# the number of possible tokens, here 10000 (1 + maximum word index (we will see it is 9999)),\n","# and the dimensionality of the embeddings, here 8.\n","# embedding_layer = Embedding(10000, 8)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ReoMuQACcZMi","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","### The Embedding layer is best understood as a dictionary:\n","- It takes as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors.\n","\n","- The Embedding layer takes as input a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers.\n","\n","- The Embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality).\n","\n","\n"]},{"metadata":{"id":"o0xBrJjRdK7g","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","### Training the Embedding layer:\n","\n","- When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just like with any other layer.\n","\n","- During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. \n","\n","- Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for."]},{"metadata":{"id":"HOPbrrI9dmmY","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","\n","## See example using the IMDB movie review sentiment prediction.\n","\n","    \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n","\n","    Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n","\n","\n","Let's quickly prepare the data. \n","\n","- We will restrict the movie reviews to the top 10,000 most common words\n","- and cut the reviews after only 20 words. \n","\n","**Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classification.**"]},{"metadata":{"colab_type":"text","id":"9yGOpaSDFsIx"},"cell_type":"markdown","source":["---\n","# <font color= #C70039 > In this example we will only use top <font color=black>300</font> words from IMBD to be able to visualize them quickly</font>"]},{"metadata":{"id":"Bxec3i_dzNPU","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.datasets import imdb\n","from keras import preprocessing\n","\n","# Number of words to consider as features\n","max_features = 300\n","INDEX_FROM = 0\n","\n","# Cut texts after this number of words \n","# (among top max_features most common words)\n","maxlen = 20\n","\n","# Load the data as lists of integers.\n","(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features,index_from=INDEX_FROM)\n","\n","# This turns our lists of integers\n","# into a 2D integer tensor of shape `(samples, maxlen)`\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B6hsahVKIpcs","colab_type":"code","colab":{}},"cell_type":"code","source":["print('Train data shape:', x_train.shape)\n","print('Test  data shape:', x_test.shape)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K1aMSv_jfYwA","colab_type":"code","colab":{}},"cell_type":"code","source":["x_train[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cxDycqaN1_o7","colab_type":"text"},"cell_type":"markdown","source":["---\n","# <font color= #C70039 > Obtain <font color=black>$word\\_to\\_id$</font> and <font color=black>$id\\_to\\_word$</font> dictionaries from IMBD</font>"]},{"metadata":{"id":"GkQ28vekzTQZ","colab_type":"code","colab":{}},"cell_type":"code","source":["word_to_id = keras.datasets.imdb.get_word_index()\n","word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n","word_to_id[\"<PAD>\"] = 0\n","word_to_id[\"<START>\"] = 1\n","word_to_id[\"<UNK>\"] = 2\n","\n","id_to_word = {value:key for key,value in word_to_id.items()}\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ld4bmckn1S8q","colab_type":"code","colab":{}},"cell_type":"code","source":["print(id_to_word[62])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qRPvpEMI1Rpt","colab_type":"code","colab":{}},"cell_type":"code","source":["print(' '.join(id_to_word[id] for id in x_train[0] ))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TQCTbYZy4MqR","colab_type":"text"},"cell_type":"markdown","source":["## Check that the maximum word index is ??? 300?"]},{"metadata":{"id":"z-e6J2QX3-bV","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","np.max(x_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"81Ms_Q8k7rdp","colab_type":"code","colab":{}},"cell_type":"code","source":["np.unique(x_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5CC9su-NJYXQ","colab_type":"text"},"cell_type":"markdown","source":["---\n","## Model with a first layer Embeddings on the input sequence + Flatten + Dense\n","\n","- Define the model\n","- Compile it\n","- Fit train data & evaluate with test data"]},{"metadata":{"id":"vy0_tt0eGfZa","colab_type":"text"},"cell_type":"markdown","source":["# <font color= #C70039 > Let's try with a vector size of dimension <font color=black>16</font> </font>"]},{"metadata":{"id":"vQxFCnnQJDNM","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense\n","\n","batch_size=32\n","\n","\n","model = Sequential()\n","\n","#### NOTE that Embedding requires input_length #######################################\n","# We specify the maximum input length to our Embedding layer\n","# so we can later flatten the embedded inputs\n","# This argument is required if you are going to connect Flatten then Dense layers upstream\n","\n","model.add(Embedding(max_features, 8, input_length=maxlen))\n","# After the Embedding layer, \n","# our activations have shape `(samples, maxlen, 8)`.\n","\n","# We flatten the 3D tensor of embeddings \n","# into a 2D tensor of shape `(samples, maxlen * 8)`\n","model.add(Flatten())\n","\n","# We add the classifier on top\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=15,\n","                    batch_size=batch_size,\n","                    validation_split=0.2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HjiE8uw-UyQk","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"AD9QRXwQTJug","colab_type":"code","colab":{}},"cell_type":"code","source":["score, acc = model.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Test score without RNN:', score)\n","print('Test accuracy without RNN:', acc)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"8ZYOiBo93aCJ"},"cell_type":"markdown","source":["---\n","# <font color= #C70039 > Get <font color=black>$embedding\\ weights$</font> from the trained <font color=black>Embeddings layer</font> in the Keras model</font>"]},{"metadata":{"id":"_HNLSPS_3Ncr","colab_type":"code","colab":{}},"cell_type":"code","source":["word_embds = model.layers[0].get_weights()[0]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fZFfdDM34qod","colab_type":"code","colab":{}},"cell_type":"code","source":["type(word_embds)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bT254lWC4aYC","colab_type":"code","colab":{}},"cell_type":"code","source":["word_embds.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aNYcNHeZHQLo","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(100, 10))\n","plt.imshow(word_embds.T,cmap='viridis')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E7TdD3S9DpS8","colab_type":"text"},"cell_type":"markdown","source":["---\n","# <font color= #C70039 > TO DO: apply t-SNE to the Embeddings layer</font>"]},{"metadata":{"id":"U9SQqJ4V5R_z","colab_type":"code","colab":{}},"cell_type":"code","source":["import time\n","\n","from sklearn.manifold import TSNE\n","\n","time_start = time.time()\n","tsne = ???\n","tsne_results = ???\n","\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sBN5NS2BEArA","colab_type":"text"},"cell_type":"markdown","source":["---\n","# <font color= #C70039 > Create a list of words for using them as labels in plots</font>"]},{"metadata":{"id":"OlHa5GcR7BCW","colab_type":"code","colab":{}},"cell_type":"code","source":["word_list = []\n","for i in range(0,max_features):\n","    word_list.append(str(id_to_word[i]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B38LW2MdEPPI","colab_type":"text"},"cell_type":"markdown","source":["---\n","# <font color= #C70039 > To DO: plot a t-SNE point and label in a figure</font>"]},{"metadata":{"id":"ojQIoKeu6A95","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 10))  # in inches\n","\n","i_to_plot=99\n","\n","x, y = ???\n","label = ???\n","\n","plt.scatter(x, y)\n","\n","plt.annotate(label,\n","                 xy=(x, y),\n","                 xytext=(5, 2),\n","                 textcoords='offset points',\n","                 ha='right',\n","                 va='bottom')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"finLhO54C_Y5","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","\n","def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n","    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n","    plt.figure(figsize=(22, 16))  # in inches\n","    for i, label in enumerate(labels):\n","        x, y = low_dim_embs[i, :]\n","        plt.scatter(x, y)\n","        plt.annotate(label,\n","                 xy=(x, y),\n","                 xytext=(5, 2),\n","                 textcoords='offset points',\n","                 ha='right',\n","                 va='bottom')\n","    plt.savefig(filename)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BTfWSfksDBGg","colab_type":"code","colab":{}},"cell_type":"code","source":["# Finally plotting and saving the fig \n","plot_with_labels(tsne_results, word_list)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pHUn6BxU_XgL","colab_type":"text"},"cell_type":"markdown","source":["---\n","# <font color= #C70039 > ... now you could try the same visualization for the LSTM + Dense model </font>"]},{"metadata":{"id":"vzyofwbIWEQM","colab_type":"text"},"cell_type":"markdown","source":["---\n","## Now test a Model with a first layer Embeddings on every input + LSTM + Dense\n","\n","- Define the model\n","- Compile it\n","- Fit train data & evaluate with test data"]},{"metadata":{"id":"dUbnUh6BU0PG","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense, LSTM\n","\n","batch_size=32\n","\n","model = Sequential()\n","\n","model.add(Embedding(10000, 8))\n","model.add(LSTM(8, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=batch_size,\n","                    validation_split=0.2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Sjb9jSFqVat4","colab_type":"code","colab":{}},"cell_type":"code","source":["score, acc = model.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Test score with LSTM:', score)\n","print('Test accuracy with LSTM:', acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"abzpUABkLURI","colab_type":"text"},"cell_type":"markdown","source":["## See Pre-trained models\n","\n","https://nlp.stanford.edu/projects/glove/\n","\n","## See Embeddings projector\n","\n","http://projector.tensorflow.org/"]}]}